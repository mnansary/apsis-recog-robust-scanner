{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Make sure the dataset in in the following structre\n","```\n","* dataset (\"/input/dataset/)\n","|-dataset_iden\n"," |-dataset_iden\n","  |-x.tfrecord\n","  |-x.tfrecord\n","  |-x.tfrecord\n","  .................\n","|-config.json\n","|-enc.h5\n","|-seq.h5\n","|-pos.h5\n","|-fuse.h5\n","```"]},{"cell_type":"markdown","metadata":{},"source":["# Change the variables in first cell accordingly"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#------------------------------\n","# change able params\n","#------------------------------\n","#--> dataset pipeline\n","PER_REPLICA_BATCH_SIZE  = 128                           # batch size per replica\n","EPOCHS                  = 100                           # number of epochs to train\n","DATASET_IDENTIFIER      = 'apsis-cdr-gen-bangla-final'  # kaggle dataset name\n","use_pretrained          = True                          # train from a pretrained version \n","eval_split              = 20                            # % of total data to use for evaluation"]},{"cell_type":"markdown","metadata":{},"source":["# Every thing is fixed from this point on"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","inp_path=f\"../input/{DATASET_IDENTIFIER}/\"\n","#--> data property\n","config_json  =  f'{inp_path}config.json'           # @path to config.json\n","#--> weights\n","# only applicable when use_pretrained is true\n","enc_weights_path        = f'{inp_path}enc.h5'  # path to \"enc.h5\"\n","seq_weights_path        = f'{inp_path}seq.h5'  # path to \"seq.h5\"\n","pos_weights_path        = f'{inp_path}pos.h5'  # path to \"pos.h5\"\n","fuse_weights_path       = f'{inp_path}fuse.h5'  # path to \"fuse.h5\"\n","\n","assert os.path.exists(config_json),\"config.json not found\"\n","if use_pretrained:\n","    assert os.path.exists(enc_weights_path ),\"enc.h5 not found\"\n","    assert os.path.exists(seq_weights_path ),\"seq.h5 not found\"\n","    assert os.path.exists(pos_weights_path ),\"pos.h5 not found\"\n","    assert os.path.exists(fuse_weights_path ),\"fuse.h5 not found\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#----------------\n","# imports\n","#---------------\n","import tensorflow as tf\n","import random\n","import json\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from glob import glob\n","from tqdm.auto import tqdm\n","from kaggle_datasets import KaggleDatasets\n","import random\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n","import tensorflow as tf\n","#-------------------\n","# fixed params\n","#------------------\n","nb_channels =  3        \n","enc_filters =  256\n","factor      =  32\n","#-------------\n","# config-globals\n","#-------------\n","with open(config_json) as f:\n","    conf = json.load(f)\n","\n","print(conf)\n","\n","img_height  =  conf[\"img_height\"]\n","img_width   =  conf[\"img_width\"]\n","vocab       =  conf[\"vocab\"]\n","pos_max     =  conf[\"pos_max\"]\n","RECORD_SIZE =  conf[\"tf_size\"] \n","zip_iden    =  conf[\"zip_iden\"]\n","\n","\n","# calculated\n","enc_shape   =  (img_height//factor,img_width//factor, enc_filters )\n","attn_shape  =  (None, enc_filters )\n","mask_len    =  int((img_width//factor)*(img_height//factor))\n","start_value    =vocab.index(\"start\")\n","end_value      =vocab.index(\"end\") \n","pad_value      =vocab.index(\"pad\")\n","\n","print(\"Label len:\",pos_max)\n","print(\"Vocab len:\",len(vocab))\n","print(\"Start value:\",start_value)\n","print(\"End value:\",end_value)\n","print(\"pad_value:\",pad_value)\n","\n","\n","\n","#--------------------------\n","# GCS Paths and tfrecords\n","#-------------------------\n","def get_tfrecs(tfrec_folder_path):\n","    gcs_pattern=os.path.join(tfrec_folder_path,'*.tfrecord')\n","    file_paths = tf.io.gfile.glob(gcs_pattern)\n","    random.shuffle(file_paths)\n","    print(f\"{tfrec_folder_path}:\",len(file_paths))\n","    return file_paths\n","\n","\n","\n","\n","GCS_PATH = KaggleDatasets().get_gcs_path(DATASET_IDENTIFIER)\n","rec_path=os.path.join(GCS_PATH,zip_iden,zip_iden) \n","recs=get_tfrecs(rec_path)\n","# dist/split\n","len_recs=len(recs)\n","nb_eval_recs=int(len_recs*(eval_split/100))\n","\n","eval_recs =recs[:nb_eval_recs]\n","train_recs=recs[nb_eval_recs:]\n","random.shuffle(eval_recs)\n","random.shuffle(train_recs)\n","\n","print(\"Eval-recs:\",len(eval_recs))\n","print(\"Train-recs:\",len(train_recs))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#----------------------------------------------------------\n","# Detect hardware, return appropriate distribution strategy\n","#----------------------------------------------------------\n","# TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    tf.config.optimizer.set_jit(True)\n","else:\n","    strategy = tf.distribute.get_strategy() \n","    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#-------------------------------------\n","# batching , strategy and steps\n","#-------------------------------------\n","if strategy.num_replicas_in_sync==1:\n","    BATCH_SIZE = PER_REPLICA_BATCH_SIZE\n","else:\n","    BATCH_SIZE = PER_REPLICA_BATCH_SIZE*strategy.num_replicas_in_sync\n","\n","# set    \n","STEPS_PER_EPOCH = (len(train_recs)*RECORD_SIZE)//BATCH_SIZE\n","EVAL_STEPS      = (len(eval_recs)*RECORD_SIZE)//BATCH_SIZE\n","print(\"Steps:\",STEPS_PER_EPOCH)\n","print(\"Batch Size:\",BATCH_SIZE)\n","print(\"Eval Steps:\",EVAL_STEPS)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#------------------------------\n","# parsing tfrecords basic\n","#------------------------------\n","def data_input_fn(recs,mode): \n","    '''\n","      This Function generates data from gcs\n","      * The parser function should look similiar now because of datasetEDA\n","    '''\n","    def _parser(example):   \n","        feature ={  'image'  : tf.io.FixedLenFeature([],tf.string) ,\n","                    'label'  : tf.io.FixedLenFeature([pos_max],tf.int64),\n","                    'mask'   : tf.io.FixedLenFeature([mask_len],tf.int64)\n","        }    \n","        parsed_example=tf.io.parse_single_example(example,feature)\n","        # image\n","        image_raw=parsed_example['image']\n","        image=tf.image.decode_png(image_raw,channels=nb_channels)\n","        image=tf.cast(image,tf.float32)/255.0\n","        image=tf.reshape(image,(img_height,img_width,nb_channels))\n","        \n","        # label\n","        label=parsed_example['label']\n","            \n","        # position\n","        pos=tf.range(0,pos_max)\n","        pos=tf.cast(pos,tf.int32)\n","        # mask\n","        mask=parsed_example['mask']\n","        mask=1-tf.cast(mask,tf.float32)\n","        mask=tf.stack([mask for _ in range(pos_max)])\n","\n","        return {\"image\":image,\"label\":tf.cast(label, tf.int32),\"pos\":pos,\"mask\":mask},tf.cast(label, tf.float32)\n","    \n","      \n","\n","    # fixed code (for almost all tfrec training)\n","    dataset = tf.data.TFRecordDataset(recs)\n","    dataset = dataset.map(_parser)\n","    dataset = dataset.shuffle(2048,reshuffle_each_iteration=True)\n","    dataset = dataset.repeat()\n","    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n","    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n","    return dataset\n","\n","train_ds  =   data_input_fn(train_recs,\"train\")\n","eval_ds  =   data_input_fn(eval_recs,\"eval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#------------------------\n","# visualizing data\n","#------------------------\n","\n","\n","print(\"---------------------------------------------------------------\")\n","print(\"visualizing data\")\n","print(\"---------------------------------------------------------------\")\n","for x,y in train_ds.take(1):\n","    data=np.squeeze(x[\"image\"][0])\n","    plt.imshow(data)\n","    plt.show()\n","    print(\"---------------------------------------------------------------\")\n","    print(\"label:\",x[\"label\"][0])\n","    print(\"---------------------------------------------------------------\")\n","    print(\"pos:\",x[\"pos\"][0])\n","    print(\"---------------------------------------------------------------\")\n","    print(\"mask:\",x[\"mask\"][0][0])\n","    print(\"---------------------------------------------------------------\")\n","    print('Image Batch Shape:',x[\"image\"].shape)\n","    print('Label Batch Shape:',x[\"label\"].shape)\n","    print('Position Batch Shape:',x[\"pos\"].shape)\n","    print('Mask Batch Shape:',x[\"mask\"].shape)\n","    print(\"---------------------------------------------------------------\")\n","    print('Target Batch Shape:',y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#-----------------------------------\n","#creating Embedding Weights\n","#-----------------------------------\n","if not use_pretrained:\n","    import torch\n","    import torch.nn as nn\n","    seq_emb              = nn.Embedding(len(vocab)+1,enc_filters, padding_idx=pad_value)\n","    seq_emb_weight       = seq_emb.weight.data.numpy()\n","    print(seq_emb_weight.shape)\n","    pos_emb              = nn.Embedding(pos_max+1,enc_filters)\n","    pos_emb_weight       = pos_emb.weight.data.numpy()\n","    print(pos_emb_weight.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DotAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","        Calculate the attention weights.\n","        q, k, v must have matching leading dimensions.\n","        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","        The mask has different shapes depending on its type(padding or look ahead)\n","        but it must be broadcastable for addition.\n","\n","        Args:\n","        q: query shape == (..., seq_len_q, depth)\n","        k: key shape == (..., seq_len_k, depth)\n","        v: value shape == (..., seq_len_v, depth_v)\n","        mask: Float tensor with shape broadcastable\n","              to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","        Returns:\n","        output\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.inf_val=-1e9\n","        \n","    def call(self,q, k, v, mask):\n","        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","       \n","        # scale matmul_qk\n","        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","        # add the mask to the scaled tensor.\n","        if mask is not None:\n","            scaled_attention_logits += (mask * self.inf_val)\n","\n","        # softmax is normalized on the last axis (seq_len_k) so that the scores\n","        # add up to 1.\n","        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf \n","\n","\n","def encoder():\n","    '''\n","    creates the encoder part:\n","    * defatult backbone : DenseNet121 **changeable\n","    args:\n","      img           : input image layer\n","        \n","    returns:\n","      enc           : channel reduced feature layer\n","\n","    '''\n","    # img input\n","    img=tf.keras.Input(shape=(img_height,img_width,nb_channels),name='image')\n","    # backbone\n","    backbone=tf.keras.applications.DenseNet121(input_tensor=img ,weights=None,include_top=False)\n","    # feat_out\n","    enc=backbone.output\n","    # enc \n","    enc=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(enc)\n","    return tf.keras.Model(inputs=img,outputs=enc,name=\"rs_encoder\")\n","\n","def seq_decoder():\n","    '''\n","    sequence attention decoder (for training)\n","    Tensorflow implementation of : \n","    https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/sequence_attention_decoder.py\n","    '''\n","    # label input\n","    gt=tf.keras.Input(shape=(pos_max,),dtype='int32',name=\"label\")\n","    # mask\n","    mask=tf.keras.Input(shape=(pos_max,mask_len),dtype='float32',name=\"mask\")\n","    # encoder\n","    enc=tf.keras.Input(shape=enc_shape,name='enc_seq')\n","    \n","    # embedding,weights=[seq_emb_weight]\n","    if use_pretrained:\n","        embedding=tf.keras.layers.Embedding(len(vocab)+1,enc_filters)(gt)\n","    else:\n","        embedding=tf.keras.layers.Embedding(len(vocab)+1,enc_filters,weights=[seq_emb_weight])(gt)\n","    # sequence layer (2xlstm)\n","    lstm=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(embedding)\n","    query=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(lstm)\n","    # attention modeling\n","    # value\n","    bs,h,w,nc=enc.shape\n","    value=tf.keras.layers.Reshape((h*w,nc))(enc)\n","    attn=DotAttention()(query,value,value,mask)\n","    return tf.keras.Model(inputs=[gt,enc,mask],outputs=attn,name=\"rs_seq_decoder\")\n"," \n","\n","\n","def pos_decoder():\n","    '''\n","    position attention decoder (for training)\n","    Tensorflow implementation of : \n","    https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/position_attention_decoder.py\n","    '''\n","    # pos input\n","    pt=tf.keras.Input(shape=(pos_max,),dtype='int32',name=\"pos\")\n","    # mask\n","    mask=tf.keras.Input(shape=(pos_max,mask_len),dtype='float32',name=\"mask\")\n","    # encoder\n","    enc=tf.keras.Input(shape=enc_shape,name='enc_pos')\n","    \n","    # embedding,weights=[pos_emb_weight]\n","    if use_pretrained:\n","        query=tf.keras.layers.Embedding(pos_max+1,enc_filters)(pt)\n","    else:\n","        query=tf.keras.layers.Embedding(pos_max+1,enc_filters,weights=[pos_emb_weight])(pt)\n","    # part-1:position_aware_module\n","    bs,h,w,nc=enc.shape\n","    value=tf.keras.layers.Reshape((h*w,nc))(enc)\n","    # sequence layer (2xlstm)\n","    lstm=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(value)\n","    x=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(lstm)\n","    x=tf.keras.layers.Reshape((h,w,nc))(x)\n","    # mixer\n","    x=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(x)\n","    x=tf.keras.layers.Activation(\"relu\")(x)\n","    key=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(x)\n","    bs,h,w,c=key.shape\n","    key=tf.keras.layers.Reshape((h*w,nc))(key)\n","    attn=DotAttention()(query,key,value,mask)\n","    return tf.keras.Model(inputs=[pt,enc,mask],outputs=attn,name=\"rs_pos_decoder\")\n","\n","def fusion():\n","    '''\n","    fuse the output of gt_attn and pt_attn \n","    '''\n","    # label input\n","    gt_attn=tf.keras.Input(shape=attn_shape,name=\"gt_attn\")\n","    # pos input\n","    pt_attn=tf.keras.Input(shape=attn_shape,name=\"pt_attn\")\n","    \n","    x=tf.keras.layers.Concatenate()([gt_attn,pt_attn])\n","    # Linear\n","    x=tf.keras.layers.Dense(enc_filters*2,activation=None)(x)\n","    # GLU\n","    xl=tf.keras.layers.Activation(\"linear\")(x)\n","    xs=tf.keras.layers.Activation(\"sigmoid\")(x)\n","    x =tf.keras.layers.Multiply()([xl,xs])\n","    # prediction\n","    x=tf.keras.layers.Dense(len(vocab),activation=None)(x)\n","    return tf.keras.Model(inputs=[gt_attn,pt_attn],outputs=x,name=\"rs_fusion\")\n","\n","with strategy.scope():\n","    rs_encoder    =  encoder()\n","    rs_seq_decoder=  seq_decoder()\n","    rs_pos_decoder=  pos_decoder()\n","    rs_fusion     =  fusion()\n","    if use_pretrained:\n","        rs_encoder.load_weights(enc_weights_path)\n","        rs_seq_decoder.load_weights(seq_weights_path)\n","        rs_pos_decoder.load_weights(pos_weights_path)\n","        rs_fusion.load_weights(fuse_weights_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    # optimizer\n","    optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n","    # loss\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","    def CE_loss(real, pred):\n","        mask = tf.math.logical_not(tf.math.equal(real, pad_value))\n","        loss_ = loss_object(real, pred)\n","        mask = tf.cast(mask, dtype=loss_.dtype)\n","        loss_ *= mask\n","        return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","    def C_acc(real, pred):\n","        accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n","        mask = tf.math.logical_not(tf.math.equal(real,pad_value))\n","        accuracies = tf.math.logical_and(mask, accuracies)\n","        accuracies = tf.cast(accuracies, dtype=tf.float32)\n","        mask = tf.cast(mask, dtype=tf.float32)\n","        return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class robust_scanner(tf.keras.Model):\n","    def __init__(self,encoder,seq_decoder,pos_decoder,fusion):\n","        super(robust_scanner, self).__init__()\n","        self.encoder     = encoder\n","        self.seq_decoder = seq_decoder\n","        self.pos_decoder = pos_decoder\n","        self.fusion      = fusion\n","        \n","    def compile(self,optimizer,loss_fn,acc):\n","        super(robust_scanner, self).compile()\n","        self.optimizer = optimizer\n","        self.loss_fn   = loss_fn\n","        self.acc       = acc\n","       \n","        \n","    def train_step(self, batch_data):\n","        data,gt= batch_data\n","        image=data[\"image\"]\n","        pos  =data[\"pos\"]\n","        mask =data[\"mask\"]\n","        # label\n","        label=tf.ones_like(gt,dtype=tf.float32)*start_value\n","        preds=[]\n","        \n","        with tf.GradientTape() as enc_tape, tf.GradientTape() as pos_dec_tape,tf.GradientTape() as seq_dec_tape,tf.GradientTape() as fusion_tape:\n","            enc    = self.encoder(image, training=True)\n","            pt_attn= self.pos_decoder({\"pos\":pos,\"enc_pos\":enc,\"mask\":mask},training=True)\n","            \n","            gt_attn= self.seq_decoder({\"label\":gt,\"enc_seq\":enc,\"mask\":mask},training=True)\n","            pred   = self.fusion({\"gt_attn\":gt_attn,\"pt_attn\":pt_attn},training=True)\n","            \n","            # loss\n","            loss = self.loss_fn(gt[:,1:],pred[:,:-1,:])\n","            # c acc\n","            char_acc=self.acc(gt[:,1:],pred[:,:-1,:])\n","            \n","        # calc gradients    \n","        enc_grads     = enc_tape.gradient(loss,self.encoder.trainable_variables)\n","        pos_dec_grads = pos_dec_tape.gradient(loss,self.pos_decoder.trainable_variables)\n","        seq_dec_grads = seq_dec_tape.gradient(loss,self.seq_decoder.trainable_variables)\n","        fusion_grads  = fusion_tape.gradient(loss,self.fusion.trainable_variables)\n","        \n","        # apply\n","        self.optimizer.apply_gradients(zip(enc_grads,self.encoder.trainable_variables))\n","        self.optimizer.apply_gradients(zip(pos_dec_grads,self.pos_decoder.trainable_variables))\n","\n","        self.optimizer.apply_gradients(zip(seq_dec_grads,self.seq_decoder.trainable_variables))\n","        self.optimizer.apply_gradients(zip(fusion_grads,self.fusion.trainable_variables))\n","\n","        \n","        return {\"loss\"    : loss,\n","                \"char_acc\": char_acc}\n","    \n","    def test_step(self, batch_data):\n","        data,gt= batch_data\n","        image=data[\"image\"]\n","        pos  =data[\"pos\"]\n","        mask =data[\"mask\"]\n","        # label\n","        label=tf.ones_like(gt,dtype=tf.float32)*start_value\n","        preds=[]\n","        \n","        enc    = self.encoder(image, training=False)\n","        pt_attn= self.pos_decoder({\"pos\":pos,\"enc_pos\":enc,\"mask\":mask},training=False)\n","        \n","        for i in range(pos_max):\n","            gt_attn=self.seq_decoder({\"label\":label,\"enc_seq\":enc,\"mask\":mask},training=False)\n","            step_gt_attn=gt_attn[:,i,:]\n","            step_pt_attn=pt_attn[:,i,:]\n","            pred=self.fusion({\"gt_attn\":step_gt_attn,\"pt_attn\":step_pt_attn},training=False)\n","            preds.append(pred)\n","            # can change on error\n","            char_out=tf.nn.softmax(pred,axis=-1)\n","            max_idx =tf.math.argmax(char_out,axis=-1)\n","            if i < pos_max - 1:\n","                label=tf.unstack(label,axis=-1)\n","                label[i+1]=tf.cast(max_idx,tf.float32)\n","                label=tf.stack(label,axis=-1)\n","                \n","        pred=tf.stack(preds,axis=1)\n","        # loss\n","        loss = self.loss_fn(gt[:,1:],pred[:,:-1,:])\n","        # c acc\n","        char_acc=self.acc(gt[:,1:],pred[:,:-1,:])\n","        \n","        \n","        return {\"loss\"    : loss,\n","                \"char_acc\": char_acc}\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    model = robust_scanner(rs_encoder,\n","                           rs_seq_decoder,\n","                           rs_pos_decoder,\n","                           rs_fusion)\n","\n","    model.compile(optimizer = optimizer,\n","                  loss_fn   = CE_loss,\n","                  acc       = C_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# reduces learning rate on plateau\n","lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(factor=0.1,\n","                                                  cooldown= 10,\n","                                                  patience=3,\n","                                                  verbose =1,\n","                                                  min_lr=0.1e-7)\n","# early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(patience=15, \n","                                                  verbose=1, \n","                                                  mode = 'auto') \n","\n","\n","class SaveBestModel(tf.keras.callbacks.Callback):\n","    def __init__(self):\n","        self.best = float('inf')\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        metric_value = logs['val_loss']\n","        if metric_value < self.best:\n","            print(f\"Loss Improved epoch:{epoch} from {self.best} to {metric_value}\")\n","            self.best = metric_value\n","            self.model.encoder.save_weights(\"enc.h5\")\n","            self.model.seq_decoder.save_weights(\"seq.h5\")\n","            self.model.pos_decoder.save_weights(\"pos.h5\")\n","            self.model.fusion.save_weights(f\"fuse.h5\")\n","            print(\"Saved Best Weights\")\n","    def set_model(self, model):\n","        self.model = model\n","            \n","model_save=SaveBestModel()\n","model_save.set_model(model)\n","callbacks= [lr_reducer,early_stopping,model_save]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history=model.fit(train_ds,\n","                  epochs=EPOCHS,\n","                  steps_per_epoch=STEPS_PER_EPOCH,\n","                  verbose=1,\n","                  validation_data=eval_ds,\n","                  validation_steps=EVAL_STEPS, \n","                  callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
